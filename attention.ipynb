{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scaledDotProduct(nn.Module):\n",
    "    '''\n",
    "        Attention(Q, K, V ) = softmax( QK^T/âˆšd_k)V \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, dim, drop=0.0):\n",
    "        super(scaledDotProduct, self).__init__()\n",
    "        #dim is (d_k) when sqrt'd it is meant to counter small gradients in large sets of queries and keys\n",
    "        self.d_k = np.sqrt(dim)\n",
    "        #Simple drop out \n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        #first two dimensions are batch and number of heads?\n",
    "        n = torch.matmul(q, k.transpose(2,3)) / self.d_k\n",
    "\n",
    "        if mask != None:\n",
    "            n = n.masked_fill_(mask==0, -1e9)\n",
    "        #Drop out referenced later in paper but not in original diagram\n",
    "        att = self.drop(F.softmax(n, -1))\n",
    "\n",
    "        out = torch.matmul(n, v)\n",
    "\n",
    "        return out, att \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled dot product attention testing\n",
    "#dim should be size of q and k\n",
    "scaled_dot = scaledDotProduct(3)\n",
    "q = torch.rand(1,1,2,3)\n",
    "k = torch.rand(1,1,2,3)\n",
    "v = torch.rand(1,1,2,4)\n",
    "\n",
    "\n",
    "scaled_dot(q,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, dims, d_k, d_v, dropout=0.0):\n",
    "        super(multiHeadedAttention, self).__init__()\n",
    "        #d_k=d_v = dims/h\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        #Pre-attention projection matrices\n",
    "        self.w_q = nn.Linear(dims, n_heads * d_k, bias=False)\n",
    "        self.w_k = nn.Linear(dims, n_heads * d_k, bias=False)\n",
    "        self.w_v = nn.Linear(dims, n_heads * d_v, bias=False)\n",
    "\n",
    "        self.att = scaledDotProduct(d_k)\n",
    "        #Final linear layer after concat and attention\n",
    "        self.fc = nn.Linear(n_heads*d_v, dims)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(dims,eps=1e-6)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, heads = self.d_k, self.d_v, self.n_heads\n",
    "        batch_len, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        res = q\n",
    "\n",
    "        #Pass through projection layers prior to attention layer batch x length of query x (nheads x value dimensionality)\n",
    "        #View as batches x len of query x numbers of heads x dimensionality to sperate out heads dimension\n",
    "        #print(q.shape)\n",
    "        q = self.w_q(q).view(batch_len, len_q, heads, d_k)\n",
    "        k = self.w_k(k).view(batch_len, len_k, heads, d_k)\n",
    "        v = self.w_v(v).view(batch_len, len_v, heads, d_v)\n",
    "\n",
    "\n",
    "        #Transpose for attention\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if mask != None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        q, attn = self.att(q, k, v, mask=mask)\n",
    "        #Move head dim back - batch x len query x heads x dimensionality\n",
    "        #Combined all heads into one - batch x len query x (heads x dimensionality)\n",
    "        q = q.transpose(1,2).contiguous().view(batch_len, len_q, -1)\n",
    "        q = self.drop(self.fc(q))\n",
    "        q += res\n",
    "\n",
    "        q = self.norm(q)\n",
    "\n",
    "        return q, attn\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heads, d_model, d_km d_v as per the paper\n",
    "multiHead = multiHeadedAttention(8, 512, 64, 64)\n",
    "#batches, dims, dimensionalityxn_heads\n",
    "q = torch.rand(1,512,512)\n",
    "k = torch.rand(1,512,512)\n",
    "v = torch.rand(1,512,512)\n",
    "\n",
    "\n",
    "multiHead(q,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionFeedFoward(nn.Module):\n",
    "    def __init__(self, inp, hid, drop=0.0):\n",
    "        super(positionFeedFoward, self).__init__()\n",
    "        self.w1 = nn.Linear(inp,hid)\n",
    "        self.w2 = nn.Linear(hid,inp)\n",
    "        self.norm = nn.LayerNorm(inp, eps=1e-6)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "\n",
    "        x = self.w2(F.relu(self.w1(x)))\n",
    "        x = self.drop(x)\n",
    "        x += res\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    '''Combinds MultiHeadedAttention and FeedForward, two layers'''\n",
    "    def __init__(self, dims, hid, nheads, d_k, d_v, drop=0.0):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attn = multiHeadedAttention(nheads, dims,d_k, d_v, dropout=drop)\n",
    "        self.ffn = positionFeedFoward(dims, hid, drop=drop)\n",
    "\n",
    "    def forward(self, inp, mask=None):\n",
    "        out, attn = self.attn(\n",
    "            inp, inp, inp, mask\n",
    "        )\n",
    "        out = self.ffn(out)\n",
    "\n",
    "        return out, attn\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    '''Combinds MultiHeadedAttention and FeeForward, three layers'''\n",
    "    def __init__(self, dims, hid, nheads, d_k, d_v, drop=0.0):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.slf_attn = multiHeadedAttention(nheads, dims,d_k, d_v, dropout=drop)\n",
    "        self.enc_attn = multiHeadedAttention(nheads, dims,d_k, d_v, dropout=drop)\n",
    "        self.ffn = positionFeedFoward(dims, hid, drop=drop)\n",
    "\n",
    "    def forward(self, inp, enc_out, slf_mask, enc_mask=None):\n",
    "        dec_out, dec_attn = self.slf_attn(\n",
    "            inp, inp, inp, slf_mask\n",
    "        )\n",
    "\n",
    "        dec_out, enc_attn = self.enc_attn(\n",
    "            dec_out, enc_out, enc_out, enc_mask\n",
    "        )\n",
    "        dec_out = self.ffn(dec_out)\n",
    "\n",
    "        return dec_out, dec_attn, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heads, d_model, d_km d_v as per the paper\n",
    "enc = EncoderLayer(512, 20, 8, 64, 64)\n",
    "#batches, dims, dimensionalityxn_heads\n",
    "q = torch.rand(1,512,512)\n",
    "k = torch.rand(1,512,512)\n",
    "v = torch.rand(1,512,512)\n",
    "\n",
    "\n",
    "enc(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytoch version adapted from here https://pub.aimind.so/creating-sinusoidal-positional-embedding-from-scratch-in-pytorch-98c49e153d6\n",
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self, hid, n_pos=200):\n",
    "        super(PosEncoding, self).__init__()\n",
    "\n",
    "        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_pos, hid))\n",
    "\n",
    "    def _get_sinusoid_encoding_table(self, n_pos, hid):\n",
    "\n",
    "        if hid %2 != 0:\n",
    "            raise ValueError(\"Sinusoidal positional embedding cannot apply to odd token embedding dim={}\".format(hid))\n",
    "        \n",
    "        positions = torch.arange(0,n_pos).unsqueeze_(1)\n",
    "        embeds = torch.zeros(n_pos, hid)\n",
    "\n",
    "        denom = torch.pow(10000, 2 * torch.arange(0, hid//2)/2)\n",
    "        embeds[:, 0::2] = torch.sin(positions/denom)\n",
    "        embeds[:, 1::2] = torch.cos(positions/denom)\n",
    "        embeds = embeds.unsqueeze(0)\n",
    "\n",
    "        return embeds\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pos_table[:, :x.size(1)].clone().detach()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''Encoder model'''\n",
    "    def __init__(\n",
    "            self, n_vocab, d_word, n_layers, n_head, d_k, d_v, dims, hid, pad, dropout=0.0, n_pos=200, scale_emb=False\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.word_emb = nn.Embedding(n_vocab, d_word, padding_idx=pad)\n",
    "        self.pos_enc = PosEncoding(d_word, n_pos=n_pos)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        self.stack = nn.ModuleList([\n",
    "            EncoderLayer(dims, hid, n_head, d_k, d_v, drop=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(dims, eps=1e-6)\n",
    "        self.scale_emb = scale_emb\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, seq, mask, ret_attns=False):\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        enc_out = self.word_emb(seq)\n",
    "        if self.scale_emb:\n",
    "            enc_out *= self.dims ** 0.5\n",
    "        enc_out = self.pos_enc(enc_out)\n",
    "        enc_out = self.drop(enc_out)\n",
    "        enc_out = self.layer_norm(enc_out)\n",
    "\n",
    "        for enc_layer in self.stack:\n",
    "            enc_out, enc_slf_attn = enc_layer(enc_out, mask=mask)\n",
    "            enc_slf_attn_list += [enc_slf_attn] if ret_attns else []\n",
    "\n",
    "        #if ret_attns:\n",
    "            #return enc_out, enc_slf_attn_list\n",
    "        return enc_out, enc_slf_attn_list\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''Decoder model'''\n",
    "    def __init__(\n",
    "            self, n_vocab, d_word, n_layers, n_head, d_k, d_v, dims, hid, pad, dropout=0.0 , n_pos=200, scale_emb=False\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.word_emb = nn.Embedding(n_vocab, d_word, padding_idx=pad)\n",
    "        self.pos_enc = PosEncoding(d_word, n_pos=n_pos)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        self.stack = nn.ModuleList([\n",
    "            DecoderLayer(dims, hid, n_head, d_k, d_v, drop=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(dims, eps=1e-6)\n",
    "        self.scale_emb = scale_emb\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, seq, mask, enc_out, src_mask, ret_attns=False):\n",
    "        dec_slf_attn_list, dec_enc_attn_list = [],[]\n",
    "\n",
    "        dec_out = self.word_emb(seq)\n",
    "        if self.scale_emb:\n",
    "            dec_out *= self.dims ** 0.5\n",
    "        dec_out = self.pos_enc(dec_out)\n",
    "        dec_out = self.drop(dec_out)\n",
    "        dec_out = self.layer_norm(dec_out)\n",
    "\n",
    "        for dec_layer in self.stack:\n",
    "            dec_out, dec_self_attn, dec_enc_attn = dec_layer(\n",
    "                dec_out, enc_out, slf_mask=mask, enc_mask=src_mask\n",
    "            )\n",
    "            dec_slf_attn_list += [dec_self_attn] if ret_attns else []\n",
    "            dec_enc_attn_list += [dec_enc_attn] if ret_attns else []\n",
    "\n",
    "        #if ret_attns:\n",
    "            #return dec_out, dec_slf_attn_list, dec_enc_attn_list\n",
    "        return dec_out, dec_slf_attn_list, dec_enc_attn_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "def get_subsequent_mask(seq):\n",
    "    ''' For masking out the subsequent info. '''\n",
    "    len_s = seq.size(-1)\n",
    "    subsequent_mask = (1 - torch.triu(\n",
    "        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    '''Attempt at recreating the sequence to sequence model'''\n",
    "    def __init__(\n",
    "            self, src_vocab, trg_vocab, src_pad, trg_pad, d_word=512, dims=512, hid=2048, n_layers=6, n_heads=8, d_k=64, d_v=64, drop=0.0, n_pos=200, trg_emb_prj_weight_sharing=True, emb_src_trg_weight_sharing=True, scale_emb_or_prj='prj'\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.src_pad, self.trg_pad = src_pad, trg_pad\n",
    "\n",
    "        assert scale_emb_or_prj in ['emb', 'prj', 'none']\n",
    "        scale_emb = (scale_emb_or_prj=='emb') if trg_emb_prj_weight_sharing else False\n",
    "        self.scale_prj = (scale_emb_or_prj == 'prj') if trg_emb_prj_weight_sharing else False\n",
    "        self.dims = dims\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            n_vocab=src_vocab, n_pos=n_pos,\n",
    "            d_word=d_word, dims=dims, hid=hid,\n",
    "            n_layers=n_layers, n_head=n_heads, d_k=d_k, d_v=d_v,\n",
    "            pad=src_pad, dropout=drop, scale_emb=scale_emb\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            n_vocab=trg_vocab, n_pos=n_pos,\n",
    "            d_word=d_word, dims=dims, hid=hid,\n",
    "            n_layers=n_layers, n_head=n_heads, d_k=d_k, d_v=d_v,\n",
    "            pad=trg_pad, dropout=drop, scale_emb=scale_emb\n",
    "        )\n",
    "\n",
    "        self.trg_word_prj = nn.Linear(dims, trg_vocab, bias=False)\n",
    "\n",
    "        for j in self.parameters():\n",
    "            if j.dim()>1:\n",
    "                nn.init.xavier_uniform_(j)\n",
    "\n",
    "        assert dims == d_word\n",
    "\n",
    "        if trg_emb_prj_weight_sharing:\n",
    "            self.trg_word_prj.weight = self.decoder.word_emb.weight\n",
    "\n",
    "        if emb_src_trg_weight_sharing:\n",
    "            self.encoder.word_emb.weight = self.decoder.word_emb.weight\n",
    "\n",
    "    def forward(self, src_seq, trg_seq):\n",
    "        src_mask = get_pad_mask(src_seq, self.src_pad)\n",
    "        trg_mask = get_pad_mask(trg_seq, self.trg_pad) & get_subsequent_mask(trg_seq)\n",
    "\n",
    "        enc_out, *_ = self.encoder(src_seq, src_mask)\n",
    "        dec_out, *_ = self.decoder(trg_seq, trg_mask, enc_out, src_mask)\n",
    "        seq_logit = self.trg_word_prj(dec_out)\n",
    "        if self.scale_prj:\n",
    "            seq_logit *= self.dims ** -0.5\n",
    "\n",
    "        return seq_logit.view(-1, seq_logit.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text file containing english sentences\n",
    "file_path = './raw_sentences.txt'\n",
    "\n",
    "sentences = []\n",
    "for line in open(file_path):\n",
    "    words = line.split()\n",
    "    sentence = [word.lower() for word in words]\n",
    "    sentences.append(sentence)\n",
    "\n",
    "vocab = set([w for s in sentences for w in s])\n",
    "\n",
    "print(len(sentences)) # 97162\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, valid, train = sentences[:10000], sentences[10000:20000], sentences[20000:]\n",
    "\n",
    "for i in range(10):\n",
    "    print(train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter()\n",
    "n=0\n",
    "sum = 0\n",
    "for n in range(len(sentences)):\n",
    "    sum+=len(sentences[n])\n",
    "    count.update(sentences[n])\n",
    "print('avg: ' + str(sum/n))\n",
    "print('unique: '+str(len(count)))\n",
    "print('10 most common: ')\n",
    "keys = sorted(count, key=count.get, reverse=True)[:10]\n",
    "print(keys)\n",
    "print(\"total words: \")\n",
    "sum = 0\n",
    "for val in list(count.values()):\n",
    "    sum+=val\n",
    "print(sum)\n",
    "count_percent ={}\n",
    "for key, val in list(count.items()):\n",
    "    count_percent[key]=round((val/sum)*100,2)\n",
    "\n",
    "\n",
    "print(\"Percentages\")\n",
    "print(count_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_itos = dict(enumerate(vocab))\n",
    "# A mapping of word => its index\n",
    "vocab_stoi = {word:index for index, word in vocab_itos.items()}\n",
    "\n",
    "def convert_words_to_indices(sents):\n",
    "    \"\"\"\n",
    "    This function takes a list of sentences (list of list of words)\n",
    "    and returns a new list with the same structure, but where each word\n",
    "    is replaced by its index in `vocab_stoi`.\n",
    "\n",
    "    Example:\n",
    "    >>> convert_words_to_indices([['one', 'in', 'five', 'are', 'over', 'here'],\n",
    "                                  ['other', 'one', 'since', 'yesterday'],\n",
    "                                  ['you']])\n",
    "    [[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]]\n",
    "    \"\"\"\n",
    "    sent_inds=[]\n",
    "    for sent in sents:\n",
    "        sent_ind = []\n",
    "        for word in sent:\n",
    "            sent_ind.append(vocab_stoi[word])\n",
    "        sent_inds.append(sent_ind)\n",
    "    return sent_inds\n",
    "\n",
    "def generate_4grams(seqs):\n",
    "    \"\"\"\n",
    "    This function takes a list of sentences (list of lists) and returns\n",
    "    a new list containing the 4-grams (four consequentively occuring words)\n",
    "    that appear in the sentences. Note that a unique 4-gram can appear multiple\n",
    "    times, one per each time that the 4-gram appears in the data parameter `seqs`.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> generate_4grams([[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]])\n",
    "    [[148, 98, 70, 23], [98, 70, 23, 154], [70, 23, 154, 89], [151, 148, 181, 246]]\n",
    "    >>> generate_4grams([[1, 1, 1, 1, 1]])\n",
    "    [[1, 1, 1, 1], [1, 1, 1, 1]]\n",
    "    \"\"\"\n",
    "\n",
    "    fourgrams =[]\n",
    "    for seq in seqs:\n",
    "        while len(seq)>3:\n",
    "            fourgrams.append(seq[:4])\n",
    "            seq=seq[1:]\n",
    "    return fourgrams\n",
    "\n",
    "def process_data(sents):\n",
    "    \"\"\"\n",
    "    This function takes a list of sentences (list of lists), and generates an\n",
    "    numpy matrix with shape [N, 4] containing indices of words in 4-grams.\n",
    "    \"\"\"\n",
    "    indices = convert_words_to_indices(sents)\n",
    "    fourgrams = generate_4grams(indices)\n",
    "    return np.array(fourgrams)\n",
    "\n",
    "print(generate_4grams([[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]]))\n",
    "train4grams = process_data(train)\n",
    "valid4grams = process_data(valid)\n",
    "test4grams = process_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4grams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    len(vocab), len(vocab), 0,0, n_heads=2,n_layers=2, n_pos=3\n",
    "    ).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inp = torch.tensor(train4grams[:2]).to(device)\n",
    "out = model(test_inp[:,:3], test_inp[:,1:])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_accuracy_torch(model, data, batch_size=5000, max_N=100000):\n",
    "    \"\"\"\n",
    "    Estimate the accuracy of the model on the data. To reduce\n",
    "    computation time, use at most `max_N` elements of `data` to\n",
    "    produce the estimate.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    N = 0\n",
    "    for i in range(0, data.shape[0], batch_size):\n",
    "        # get a batch of data\n",
    "        xs, ts = data[i:i+batch_size,:3], data[i:i+batch_size,3]\n",
    "        \n",
    "        # forward pass prediction\n",
    "        z = model(torch.Tensor(xs).long().to(device),torch.Tensor(xs).long().to(device))\n",
    "        z = z.cpu().detach().numpy() # convert the PyTorch tensor => numpy array\n",
    "        pred = np.argmax(z, axis=1)\n",
    "        correct += np.sum(pred == ts)\n",
    "        N += ts.shape[0]\n",
    "\n",
    "        if N > max_N:\n",
    "            break\n",
    "    return correct / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data=train4grams, validation_data = valid4grams, batch_size=300, lr=0.003, weight_decay=0, max_iters=2500, checkpoint_path=None):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    iters, losses = [], []\n",
    "    iters_sub, train_accs, val_accs  = [], [] ,[]\n",
    "\n",
    "    n = 0\n",
    "\n",
    "    while True:\n",
    "        for i in range(0, train_data.shape[0], batch_size):\n",
    "            if (i + batch_size) > train_data.shape[0]:\n",
    "                break\n",
    "            xs, ts = train_data[i:i+batch_size,:3], train_data[i:i+batch_size,3]\n",
    "\n",
    "            # convert from numpy arrays to PyTorch tensors\n",
    "            xs = torch.Tensor(xs).long().to(device)\n",
    "            ts = torch.Tensor(ts).long().to(device)\n",
    "\n",
    "            zs = model(xs,xs)[:batch_size]\n",
    "            loss = criterion(zs, ts) # compute the total loss\n",
    "            loss.backward()          # compute updates for each parameter\n",
    "            optimizer.step()         # make the updates for each parameter\n",
    "            optimizer.zero_grad()    # a clean up step for PyTorch\n",
    "\n",
    "            # save the current training information\n",
    "            iters.append(n)\n",
    "            losses.append(float(loss)/batch_size)  # compute *average* loss\n",
    "\n",
    "            if n % 500 == 0:\n",
    "                iters_sub.append(n)\n",
    "                train_cost = float(loss.cpu().detach().numpy())\n",
    "                train_acc = estimate_accuracy_torch(model, train_data)\n",
    "                train_accs.append(train_acc)\n",
    "                val_acc = estimate_accuracy_torch(model, validation_data)\n",
    "                val_accs.append(val_acc)\n",
    "                print(\"Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (\n",
    "                      n, val_acc * 100, train_acc * 100, train_cost))\n",
    "\n",
    "                if (checkpoint_path is not None) and n > 0:\n",
    "                    torch.save(model.state_dict(), checkpoint_path.format(n))\n",
    "\n",
    "            # increment the iteration number\n",
    "            n += 1\n",
    "\n",
    "            if n > max_iters:\n",
    "                return iters, losses, iters_sub, train_accs, val_accs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(iters, losses, iters_sub, train_accs, val_accs):\n",
    "    \"\"\"\n",
    "    Plot the learning curve.\n",
    "    \"\"\"\n",
    "    plt.title(\"Learning Curve: Loss per Iteration\")\n",
    "    plt.plot(iters, losses, label=\"Train\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
    "    plt.plot(iters_sub, train_accs, label=\"Train\")\n",
    "    plt.plot(iters_sub, val_accs, label=\"Validation\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = train(model)\n",
    "plot_learning_curve(*train_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
