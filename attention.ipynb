{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scaledDotProduct(nn.Module):\n",
    "    '''\n",
    "        Attention(Q, K, V ) = softmax( QK^T/âˆšd_k)V \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, dim, drop=0.0):\n",
    "        super(scaledDotProduct, self).__init__()\n",
    "        #dim is (d_k) when sqrt'd it is meant to counter small gradients in large sets of queries and keys\n",
    "        self.d_k = np.sqrt(dim)\n",
    "        #Simple drop out \n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        #first two dimensions are batch and number of heads?\n",
    "        n = torch.matmul(q, k.transpose(2,3)) / self.d_k\n",
    "\n",
    "        if mask:\n",
    "            n = n.masked_fill_(mask==0, -1e9)\n",
    "        #Drop out referenced later in paper but not in original diagram\n",
    "        att = self.drop(F.softmax(n, -1))\n",
    "\n",
    "        out = torch.matmul(n, v)\n",
    "\n",
    "        return out, att \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0842, 0.1299, 0.1133, 0.3195],\n",
       "           [0.2432, 0.3760, 0.3280, 0.9247]]]]),\n",
       " tensor([[[[0.5121, 0.4879],\n",
       "           [0.5356, 0.4644]]]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scaled dot product attention testing\n",
    "#dim should be size of q and k\n",
    "scaled_dot = scaledDotProduct(3)\n",
    "q = torch.rand(1,1,2,3)\n",
    "k = torch.rand(1,1,2,3)\n",
    "v = torch.rand(1,1,2,4)\n",
    "\n",
    "\n",
    "scaled_dot(q,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, dims, d_k, d_v, dropout=0.0):\n",
    "        super(multiHeadedAttention, self).__init__()\n",
    "        #d_k=d_v = dims/h\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        #Pre-attention projection matrices\n",
    "        self.w_q = nn.Linear(dims, n_heads * d_k, bias=False)\n",
    "        self.w_k = nn.Linear(dims, n_heads * d_k, bias=False)\n",
    "        self.w_v = nn.Linear(dims, n_heads * d_v, bias=False)\n",
    "\n",
    "        self.att = scaledDotProduct(d_k)\n",
    "        #Final linear layer after concat and attention\n",
    "        self.fc = nn.Linear(n_heads*d_v, dims)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(dims,eps=1e-6)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, heads = self.d_k, self.d_v, self.n_heads\n",
    "        batch_len, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        res = q\n",
    "\n",
    "        #Pass through projection layers prior to attention layer batch x length of query x (nheads x value dimensionality)\n",
    "        #View as batches x len of query x numbers of heads x dimensionality to sperate out heads dimension\n",
    "        print(q.shape)\n",
    "        q = self.w_q(q).view(batch_len, len_q, heads, d_k)\n",
    "        k = self.w_k(k).view(batch_len, len_k, heads, d_k)\n",
    "        v = self.w_v(v).view(batch_len, len_v, heads, d_v)\n",
    "\n",
    "\n",
    "        #Transpose for attention\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if mask:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        q, attn = self.att(q, k, v, mask=mask)\n",
    "        #Move head dim back - batch x len query x heads x dimensionality\n",
    "        #Combined all heads into one - batch x len query x (heads x dimensionality)\n",
    "        q = q.transpose(1,2).contiguous().view(batch_len, len_q, -1)\n",
    "        q = self.drop(self.fc(q))\n",
    "        q += res\n",
    "\n",
    "        q = self.norm(q)\n",
    "\n",
    "        return q, attn\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6771, -1.7764, -0.8633,  ...,  1.5172, -0.7129,  1.2825],\n",
       "          [-0.1932, -0.1158,  1.4648,  ..., -0.8946,  0.8421,  0.3897],\n",
       "          [-1.0017, -2.1038,  0.0425,  ..., -0.7560, -0.7943,  1.2451],\n",
       "          ...,\n",
       "          [ 0.9047, -1.7641,  1.1018,  ..., -1.2272,  0.8404,  2.0151],\n",
       "          [ 0.6745, -0.8764,  0.3654,  ...,  0.6772,  1.0115,  2.1145],\n",
       "          [-0.4383, -1.6716, -0.0755,  ...,  0.8128, -0.7611,  0.4531]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[[0.0019, 0.0019, 0.0020,  ..., 0.0017, 0.0019, 0.0020],\n",
       "           [0.0019, 0.0019, 0.0019,  ..., 0.0018, 0.0020, 0.0020],\n",
       "           [0.0019, 0.0019, 0.0019,  ..., 0.0018, 0.0018, 0.0020],\n",
       "           ...,\n",
       "           [0.0019, 0.0019, 0.0019,  ..., 0.0017, 0.0019, 0.0020],\n",
       "           [0.0019, 0.0019, 0.0019,  ..., 0.0017, 0.0019, 0.0019],\n",
       "           [0.0020, 0.0019, 0.0019,  ..., 0.0018, 0.0019, 0.0019]],\n",
       " \n",
       "          [[0.0020, 0.0021, 0.0019,  ..., 0.0020, 0.0020, 0.0022],\n",
       "           [0.0020, 0.0021, 0.0019,  ..., 0.0019, 0.0019, 0.0021],\n",
       "           [0.0019, 0.0020, 0.0020,  ..., 0.0019, 0.0020, 0.0022],\n",
       "           ...,\n",
       "           [0.0020, 0.0021, 0.0019,  ..., 0.0019, 0.0020, 0.0021],\n",
       "           [0.0020, 0.0020, 0.0019,  ..., 0.0019, 0.0020, 0.0022],\n",
       "           [0.0020, 0.0020, 0.0019,  ..., 0.0020, 0.0018, 0.0022]],\n",
       " \n",
       "          [[0.0019, 0.0020, 0.0019,  ..., 0.0019, 0.0021, 0.0018],\n",
       "           [0.0019, 0.0021, 0.0019,  ..., 0.0020, 0.0021, 0.0019],\n",
       "           [0.0018, 0.0019, 0.0019,  ..., 0.0020, 0.0022, 0.0019],\n",
       "           ...,\n",
       "           [0.0019, 0.0020, 0.0019,  ..., 0.0019, 0.0021, 0.0018],\n",
       "           [0.0018, 0.0020, 0.0020,  ..., 0.0020, 0.0021, 0.0019],\n",
       "           [0.0017, 0.0020, 0.0020,  ..., 0.0021, 0.0020, 0.0020]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.0019, 0.0019, 0.0020,  ..., 0.0019, 0.0020, 0.0019],\n",
       "           [0.0020, 0.0019, 0.0019,  ..., 0.0019, 0.0020, 0.0018],\n",
       "           [0.0021, 0.0019, 0.0020,  ..., 0.0018, 0.0020, 0.0016],\n",
       "           ...,\n",
       "           [0.0020, 0.0018, 0.0021,  ..., 0.0018, 0.0020, 0.0017],\n",
       "           [0.0020, 0.0019, 0.0020,  ..., 0.0019, 0.0020, 0.0018],\n",
       "           [0.0020, 0.0019, 0.0020,  ..., 0.0018, 0.0021, 0.0018]],\n",
       " \n",
       "          [[0.0020, 0.0019, 0.0019,  ..., 0.0019, 0.0019, 0.0020],\n",
       "           [0.0020, 0.0020, 0.0020,  ..., 0.0021, 0.0019, 0.0021],\n",
       "           [0.0019, 0.0020, 0.0019,  ..., 0.0020, 0.0020, 0.0021],\n",
       "           ...,\n",
       "           [0.0020, 0.0019, 0.0019,  ..., 0.0020, 0.0019, 0.0021],\n",
       "           [0.0020, 0.0020, 0.0019,  ..., 0.0020, 0.0020, 0.0022],\n",
       "           [0.0020, 0.0019, 0.0019,  ..., 0.0020, 0.0020, 0.0020]],\n",
       " \n",
       "          [[0.0022, 0.0019, 0.0018,  ..., 0.0021, 0.0020, 0.0017],\n",
       "           [0.0020, 0.0019, 0.0019,  ..., 0.0020, 0.0020, 0.0017],\n",
       "           [0.0020, 0.0020, 0.0018,  ..., 0.0020, 0.0019, 0.0017],\n",
       "           ...,\n",
       "           [0.0021, 0.0020, 0.0018,  ..., 0.0020, 0.0020, 0.0017],\n",
       "           [0.0021, 0.0019, 0.0017,  ..., 0.0020, 0.0020, 0.0018],\n",
       "           [0.0020, 0.0020, 0.0019,  ..., 0.0020, 0.0020, 0.0017]]]],\n",
       "        grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#heads, d_model, d_km d_v as per the paper\n",
    "multiHead = multiHeadedAttention(8, 512, 64, 64)\n",
    "#batches, dims, dimensionalityxn_heads\n",
    "q = torch.rand(1,512,512)\n",
    "k = torch.rand(1,512,512)\n",
    "v = torch.rand(1,512,512)\n",
    "\n",
    "\n",
    "multiHead(q,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionFeedFoward(nn.Module):\n",
    "    def __init__(self, inp, hid, drop=0.0):\n",
    "        super(positionFeedFoward, self).__init__()\n",
    "        self.w1 = nn.Linear(inp,hid)\n",
    "        self.w2 = nn.Linear(hid,inp)\n",
    "        self.norm = nn.LayerNorm(inp, eps=1e-6)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "\n",
    "        x = self.w2(F.relu(self.w1(x)))\n",
    "        x = self.drop(x)\n",
    "        x += res\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
