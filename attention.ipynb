{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scaledDotProduct(nn.Module):\n",
    "    '''\n",
    "        Attention(Q, K, V ) = softmax( QK^T/âˆšd_k)V \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, dim, drop=0.0):\n",
    "        super(scaledDotProduct, self).__init__()\n",
    "        #dim is (d_k) when sqrt'd it is meant to counter small gradients in large sets of queries and keys\n",
    "        self.d_k = np.sqrt(dim)\n",
    "        #Simple drop out \n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        #first two dimensions are batch and number of heads?\n",
    "        n = torch.matmul(q, k.transpose(2,3)) / self.d_k\n",
    "\n",
    "        if mask:\n",
    "            n = n.masked_fill_(mask==0, -1e9)\n",
    "        #Drop out referenced later in paper but not in original diagram\n",
    "        att = self.drop(F.softmax(n, -1))\n",
    "\n",
    "        out = torch.matmul(n, v)\n",
    "\n",
    "        return out, att \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.3464, 0.2510, 0.2729, 0.0916],\n",
       "           [0.3734, 0.2844, 0.3056, 0.1029]]]]),\n",
       " tensor([[[[0.4680, 0.5320],\n",
       "           [0.4710, 0.5290]]]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scaled dot product attention testing\n",
    "#dim should be size of q and k\n",
    "scaled_dot = scaledDotProduct(3)\n",
    "q = torch.rand(1,1,2,3)\n",
    "k = torch.rand(1,1,2,3)\n",
    "v = torch.rand(1,1,2,4)\n",
    "\n",
    "\n",
    "scaled_dot(q,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, dims, d_k, d_v, dropout=0.0):\n",
    "        super(multiHeadedAttention, self).__init__()\n",
    "        #d_k=d_v = dims/h\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        #Pre-attention projection matrices\n",
    "        self.w_q = nn.Linear(dims, n_heads * d_k, bias=False)\n",
    "        self.w_k = nn.Linear(dims, n_heads * d_k, bias=False)\n",
    "        self.w_v = nn.Linear(dims, n_heads * d_v, bias=False)\n",
    "\n",
    "        self.att = scaledDotProduct(d_k)\n",
    "        #Final linear layer after concat and attention\n",
    "        self.fc = nn.Linear(n_heads*d_v, dims)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(dims,eps=1e-6)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, heads = self.d_k, self.d_v, self.n_heads\n",
    "        batch_len, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        res = q\n",
    "\n",
    "        #Pass through projection layers prior to attention layer batch x length of query x (nheads x value dimensionality)\n",
    "        #View as batches x len of query x numbers of heads x dimensionality to sperate out heads dimension\n",
    "        print(q.shape)\n",
    "        q = self.w_q(q).view(batch_len, len_q, heads, d_k)\n",
    "        k = self.w_k(k).view(batch_len, len_k, heads, d_k)\n",
    "        v = self.w_v(v).view(batch_len, len_v, heads, d_v)\n",
    "\n",
    "\n",
    "        #Transpose for attention\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if mask:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        q, attn = self.att(q, k, v, mask=mask)\n",
    "        #Move head dim back - batch x len query x heads x dimensionality\n",
    "        #Combined all heads into one - batch x len query x (heads x dimensionality)\n",
    "        q = q.transpose(1,2).contiguous().view(batch_len, len_q, -1)\n",
    "        q = self.drop(self.fc(q))\n",
    "        q += res\n",
    "\n",
    "        q = self.norm(q)\n",
    "\n",
    "        return q, attn\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.5688,  1.4184,  0.3202,  ..., -1.0021, -0.2049, -0.3221],\n",
       "          [-0.7246,  1.0414,  1.2804,  ..., -1.4171,  0.1137, -0.1924],\n",
       "          [-0.7711,  0.8162,  1.2456,  ..., -1.2875,  0.1525, -0.1983],\n",
       "          ...,\n",
       "          [-1.8663,  1.2953,  1.9278,  ..., -0.9809, -0.2540, -0.2188],\n",
       "          [-1.0484,  1.1081,  1.3690,  ..., -1.3603,  0.0067, -0.0589],\n",
       "          [-0.9783,  1.1937,  1.2425,  ..., -1.3367, -0.2213, -0.0561]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[[0.0019, 0.0022, 0.0020,  ..., 0.0019, 0.0019, 0.0021],\n",
       "           [0.0018, 0.0021, 0.0019,  ..., 0.0019, 0.0020, 0.0020],\n",
       "           [0.0018, 0.0021, 0.0020,  ..., 0.0018, 0.0019, 0.0020],\n",
       "           ...,\n",
       "           [0.0018, 0.0022, 0.0020,  ..., 0.0020, 0.0019, 0.0021],\n",
       "           [0.0018, 0.0021, 0.0020,  ..., 0.0018, 0.0020, 0.0022],\n",
       "           [0.0018, 0.0022, 0.0019,  ..., 0.0019, 0.0019, 0.0021]],\n",
       " \n",
       "          [[0.0020, 0.0021, 0.0020,  ..., 0.0021, 0.0021, 0.0021],\n",
       "           [0.0020, 0.0021, 0.0020,  ..., 0.0020, 0.0020, 0.0021],\n",
       "           [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0021, 0.0021],\n",
       "           ...,\n",
       "           [0.0020, 0.0020, 0.0021,  ..., 0.0022, 0.0022, 0.0021],\n",
       "           [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0021, 0.0021],\n",
       "           [0.0020, 0.0020, 0.0020,  ..., 0.0021, 0.0021, 0.0021]],\n",
       " \n",
       "          [[0.0018, 0.0019, 0.0019,  ..., 0.0019, 0.0018, 0.0019],\n",
       "           [0.0018, 0.0020, 0.0019,  ..., 0.0019, 0.0018, 0.0019],\n",
       "           [0.0019, 0.0019, 0.0019,  ..., 0.0020, 0.0017, 0.0019],\n",
       "           ...,\n",
       "           [0.0018, 0.0020, 0.0019,  ..., 0.0020, 0.0018, 0.0020],\n",
       "           [0.0017, 0.0021, 0.0019,  ..., 0.0020, 0.0017, 0.0019],\n",
       "           [0.0018, 0.0020, 0.0020,  ..., 0.0020, 0.0018, 0.0019]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.0019, 0.0017, 0.0019,  ..., 0.0018, 0.0019, 0.0021],\n",
       "           [0.0021, 0.0018, 0.0020,  ..., 0.0018, 0.0020, 0.0020],\n",
       "           [0.0020, 0.0018, 0.0019,  ..., 0.0019, 0.0020, 0.0020],\n",
       "           ...,\n",
       "           [0.0020, 0.0019, 0.0020,  ..., 0.0019, 0.0019, 0.0021],\n",
       "           [0.0019, 0.0018, 0.0019,  ..., 0.0018, 0.0019, 0.0021],\n",
       "           [0.0020, 0.0018, 0.0019,  ..., 0.0019, 0.0020, 0.0021]],\n",
       " \n",
       "          [[0.0021, 0.0019, 0.0017,  ..., 0.0017, 0.0018, 0.0020],\n",
       "           [0.0021, 0.0019, 0.0018,  ..., 0.0018, 0.0018, 0.0021],\n",
       "           [0.0021, 0.0019, 0.0018,  ..., 0.0018, 0.0018, 0.0021],\n",
       "           ...,\n",
       "           [0.0021, 0.0019, 0.0019,  ..., 0.0019, 0.0018, 0.0020],\n",
       "           [0.0021, 0.0019, 0.0017,  ..., 0.0019, 0.0018, 0.0020],\n",
       "           [0.0022, 0.0018, 0.0018,  ..., 0.0017, 0.0018, 0.0019]],\n",
       " \n",
       "          [[0.0020, 0.0022, 0.0019,  ..., 0.0019, 0.0019, 0.0018],\n",
       "           [0.0020, 0.0020, 0.0019,  ..., 0.0018, 0.0019, 0.0019],\n",
       "           [0.0021, 0.0021, 0.0018,  ..., 0.0018, 0.0018, 0.0018],\n",
       "           ...,\n",
       "           [0.0020, 0.0021, 0.0019,  ..., 0.0018, 0.0019, 0.0018],\n",
       "           [0.0020, 0.0021, 0.0019,  ..., 0.0020, 0.0019, 0.0018],\n",
       "           [0.0020, 0.0021, 0.0019,  ..., 0.0018, 0.0018, 0.0019]]]],\n",
       "        grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#heads, d_model, d_km d_v as per the paper\n",
    "multiHead = multiHeadedAttention(8, 512, 64, 64)\n",
    "#batches, dims, dimensionalityxn_heads\n",
    "q = torch.rand(1,512,512)\n",
    "k = torch.rand(1,512,512)\n",
    "v = torch.rand(1,512,512)\n",
    "\n",
    "\n",
    "multiHead(q,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionFeedFoward(nn.Module):\n",
    "    def __init__(self, inp, hid, drop=0.0):\n",
    "        super(positionFeedFoward, self).__init__()\n",
    "        self.w1 = nn.Linear(inp,hid)\n",
    "        self.w2 = nn.Linear(hid,inp)\n",
    "        self.norm = nn.LayerNorm(inp, eps=1e-6)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "\n",
    "        x = self.w2(F.relu(self.w1(x)))\n",
    "        x = self.drop(x)\n",
    "        x += res\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    '''Combinds MultiHeadedAttention and FeedForward, two layers'''\n",
    "    def __init__(self, dims, hid, nheads, d_k, d_v, drop=0.0):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attn = multiHeadedAttention(nheads, dims,d_k, d_v, dropout=drop)\n",
    "        self.ffn = positionFeedFoward(dims, hid, drop=drop)\n",
    "\n",
    "    def forward(self, inp, mask=None):\n",
    "        out, attn = self.attn(\n",
    "            inp, inp, inp, mask\n",
    "        )\n",
    "        out = self.ffn(out)\n",
    "\n",
    "        return out, attn\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    '''Combinds MultiHeadedAttention and FeeForward, three layers'''\n",
    "    def __init__(self, dims, hid, nheads, d_k, d_v, drop=0.0):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_attn = multiHeadedAttention(nheads, dims,d_k, d_v, dropout=drop)\n",
    "        self.enc_attn = multiHeadedAttention(nheads, dims,d_k, d_v, dropout=drop)\n",
    "        self.ffn = positionFeedFoward(dims, hid, drop=drop)\n",
    "\n",
    "    def forward(self, inp, enc_out, slf_mask, enc_mask=None):\n",
    "        dec_out, dec_attn = self.slf_attn(\n",
    "            inp, inp, inp, slf_mask\n",
    "        )\n",
    "\n",
    "        dec_out, enc_attn = self.enc_attn(\n",
    "            dec_out, enc_out, enc_out, enc_mask\n",
    "        )\n",
    "        dec_out = self.ffn(dec_out)\n",
    "\n",
    "        return dec_out, dec_attn, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.9075,  0.3117,  0.8335,  ..., -0.0801, -0.3563, -0.8519],\n",
       "          [ 0.5633, -0.3112, -0.0830,  ..., -0.6547, -0.8622, -0.9671],\n",
       "          [ 0.4893,  1.1514, -1.5643,  ..., -0.2486, -1.9703,  0.0916],\n",
       "          ...,\n",
       "          [ 0.3537,  0.3940, -0.5105,  ..., -0.5003, -1.7616, -1.0855],\n",
       "          [-0.2221,  0.1675, -0.5074,  ...,  0.5400, -1.4164,  0.1038],\n",
       "          [-0.1265,  0.8833, -0.6364,  ..., -0.2704, -1.9819, -0.3983]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[[0.0019, 0.0018, 0.0020,  ..., 0.0018, 0.0020, 0.0020],\n",
       "           [0.0020, 0.0017, 0.0020,  ..., 0.0018, 0.0020, 0.0020],\n",
       "           [0.0020, 0.0018, 0.0020,  ..., 0.0019, 0.0020, 0.0020],\n",
       "           ...,\n",
       "           [0.0020, 0.0018, 0.0019,  ..., 0.0018, 0.0021, 0.0021],\n",
       "           [0.0020, 0.0018, 0.0019,  ..., 0.0019, 0.0019, 0.0020],\n",
       "           [0.0020, 0.0018, 0.0020,  ..., 0.0018, 0.0019, 0.0020]],\n",
       " \n",
       "          [[0.0019, 0.0021, 0.0019,  ..., 0.0019, 0.0019, 0.0019],\n",
       "           [0.0018, 0.0019, 0.0020,  ..., 0.0020, 0.0019, 0.0019],\n",
       "           [0.0018, 0.0020, 0.0019,  ..., 0.0019, 0.0020, 0.0019],\n",
       "           ...,\n",
       "           [0.0019, 0.0019, 0.0020,  ..., 0.0020, 0.0019, 0.0019],\n",
       "           [0.0018, 0.0019, 0.0019,  ..., 0.0020, 0.0019, 0.0019],\n",
       "           [0.0019, 0.0019, 0.0020,  ..., 0.0020, 0.0019, 0.0020]],\n",
       " \n",
       "          [[0.0021, 0.0018, 0.0019,  ..., 0.0021, 0.0020, 0.0020],\n",
       "           [0.0020, 0.0019, 0.0018,  ..., 0.0021, 0.0021, 0.0021],\n",
       "           [0.0021, 0.0020, 0.0018,  ..., 0.0021, 0.0020, 0.0021],\n",
       "           ...,\n",
       "           [0.0020, 0.0019, 0.0019,  ..., 0.0021, 0.0020, 0.0021],\n",
       "           [0.0021, 0.0019, 0.0019,  ..., 0.0021, 0.0020, 0.0020],\n",
       "           [0.0020, 0.0019, 0.0019,  ..., 0.0021, 0.0021, 0.0020]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.0020, 0.0020, 0.0019,  ..., 0.0020, 0.0019, 0.0019],\n",
       "           [0.0020, 0.0021, 0.0019,  ..., 0.0021, 0.0019, 0.0018],\n",
       "           [0.0020, 0.0020, 0.0019,  ..., 0.0020, 0.0020, 0.0018],\n",
       "           ...,\n",
       "           [0.0021, 0.0020, 0.0019,  ..., 0.0021, 0.0019, 0.0018],\n",
       "           [0.0020, 0.0019, 0.0019,  ..., 0.0021, 0.0018, 0.0018],\n",
       "           [0.0021, 0.0020, 0.0019,  ..., 0.0021, 0.0019, 0.0018]],\n",
       " \n",
       "          [[0.0021, 0.0020, 0.0018,  ..., 0.0019, 0.0020, 0.0021],\n",
       "           [0.0020, 0.0020, 0.0017,  ..., 0.0018, 0.0020, 0.0020],\n",
       "           [0.0021, 0.0020, 0.0018,  ..., 0.0019, 0.0020, 0.0020],\n",
       "           ...,\n",
       "           [0.0021, 0.0019, 0.0017,  ..., 0.0018, 0.0020, 0.0020],\n",
       "           [0.0020, 0.0018, 0.0018,  ..., 0.0019, 0.0021, 0.0021],\n",
       "           [0.0021, 0.0020, 0.0018,  ..., 0.0019, 0.0020, 0.0021]],\n",
       " \n",
       "          [[0.0022, 0.0020, 0.0021,  ..., 0.0019, 0.0021, 0.0020],\n",
       "           [0.0020, 0.0021, 0.0021,  ..., 0.0019, 0.0020, 0.0019],\n",
       "           [0.0021, 0.0021, 0.0021,  ..., 0.0018, 0.0019, 0.0021],\n",
       "           ...,\n",
       "           [0.0020, 0.0020, 0.0020,  ..., 0.0019, 0.0019, 0.0019],\n",
       "           [0.0021, 0.0021, 0.0020,  ..., 0.0018, 0.0019, 0.0020],\n",
       "           [0.0022, 0.0020, 0.0020,  ..., 0.0019, 0.0019, 0.0019]]]],\n",
       "        grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#heads, d_model, d_km d_v as per the paper\n",
    "enc = EncoderLayer(512, 20, 8, 64, 64)\n",
    "#batches, dims, dimensionalityxn_heads\n",
    "q = torch.rand(1,512,512)\n",
    "k = torch.rand(1,512,512)\n",
    "v = torch.rand(1,512,512)\n",
    "\n",
    "\n",
    "enc(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytoch version adapted from here https://pub.aimind.so/creating-sinusoidal-positional-embedding-from-scratch-in-pytorch-98c49e153d6\n",
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self, hid, n_pos=200):\n",
    "        super(PosEncoding, self).__init__()\n",
    "\n",
    "        self.register_buffer('table', self._get_sinusoid_encoding_table(n_pos, hid))\n",
    "\n",
    "    def _get_sinusoid_encoding_table(self, n_pos, hid):\n",
    "\n",
    "        if hid %2 != 0:\n",
    "            raise ValueError(\"Sinusoidal positional embedding cannot apply to odd token embedding dim={}\".format(hid))\n",
    "        \n",
    "        positions = torch.arange(0,n_pos).unsqueeze_(1)\n",
    "        embeds = torch.zeros(n_pos, hid)\n",
    "\n",
    "        denom = torch.pow(10000, 2 * torch.arange(0, hid//2)/2)\n",
    "        embeds[:, 0::2] = torch.sin(positions/denom)\n",
    "        embeds[:, 1::2] = torch.cos(positions/denom)\n",
    "\n",
    "        return embeds\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pos_table[:, :x.size(1)].clone().detach()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''Encoder model'''\n",
    "    def __init__(\n",
    "            self, n_vocab, d_word, n_layers, n_head, d_k, d_v, dims, hid, pad, dropout=0.0, n_pos=200, scale_emb=False\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.word_emb = nn.Embedding(n_vocab, d_word, padding_idx=pad)\n",
    "        self.pos_enc = PosEncoding(d_word, n_pos=n_pos)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        self.stack = nn.ModuleList([\n",
    "            EncoderLayer(dims, hid, n_head, d_k, d_v, drop=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(dims, eps=1e-6)\n",
    "        self.scale_emb = scale_emb\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, seq, mask, ret_attns=False):\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        enc_out = self.word_emb(seq)\n",
    "        if self.scale_emb:\n",
    "            enc_out *= self.dims ** 0.5\n",
    "        enc_out = self.pos_enc(enc_out)\n",
    "        enc_out = self.drop(enc_out)\n",
    "        enc_out = self.layer_norm(enc_out)\n",
    "\n",
    "        for enc_layer in self.stack:\n",
    "            enc_out, enc_slf_attn = enc_layer(enc_out, mask=mask)\n",
    "            enc_slf_attn_list += [enc_slf_attn] if ret_attns else []\n",
    "\n",
    "        if ret_attns:\n",
    "            return enc_out, enc_slf_attn_list\n",
    "        return enc_out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''Decoder model'''\n",
    "    def __init__(\n",
    "            self, n_vocab, d_word, n_layers, n_head, d_k, d_v, dims, hid, pad, dropout=0.0 , n_pos=200, scale_emb=False\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.word_emb = nn.Embedding(n_vocab, d_word, padding_idx=pad)\n",
    "        self.pos_enc = PosEncoding(d_word, n_pos=n_pos)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        self.stack = nn.ModuleList([\n",
    "            DecoderLayer(dims, hid, n_head, d_k, d_v, drop=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(dims, eps=1e-6)\n",
    "        self.scale_emb = scale_emb\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, seq, mask, enc_out, src_mask, ret_attns=False):\n",
    "        dec_slf_attn_list, dec_enc_attn_list = [],[]\n",
    "\n",
    "        dec_out = self.word_emb(seq)\n",
    "        if self.scale_emb:\n",
    "            dec_out *= self.dims ** 0.5\n",
    "        dec_out = self.pos_enc(dec_out)\n",
    "        dec_out = self.drop(dec_out)\n",
    "        dec_out = self.layer_norm(dec_out)\n",
    "\n",
    "        for dec_layer in self.stack:\n",
    "            dec_out, dec_self_attn, dec_enc_attn = dec_layer(\n",
    "                dec_out, enc_out, slf_mask=mask, enc_mask=src_mask\n",
    "            )\n",
    "            dec_slf_attn_list += [dec_self_attn] if ret_attns else []\n",
    "            dec_enc_attn_list += [dec_enc_attn] if ret_attns else []\n",
    "\n",
    "        if ret_attns:\n",
    "            return dec_out, dec_slf_attn_list, dec_enc_attn_list\n",
    "        return dec_out\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
