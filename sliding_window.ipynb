{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 50 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "\n",
    "win_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chunk(x, w):\n",
    "    '''convert x into overlapping chunks. Chunk size = 2w, overlap size = w'''\n",
    "    x = x.view(x.size(0), x.size(1)// (w * 2), w * 2, x.size(2))\n",
    "    chunk_size = list(x.size())\n",
    "    chunk_size[1] = chunk_size[1] * 2 - 1\n",
    "    chunk_stride = list(x.stride())\n",
    "    chunk_stride[1] = chunk_stride[1] // 2\n",
    "\n",
    "    return x.as_strided(size=chunk_size, stride=chunk_stride)\n",
    "\n",
    "def _skew(x, dir, pad):\n",
    "    '''Convert diagonals into columns'''\n",
    "    x_pad = F.pad(x, dir, value=pad)\n",
    "    x_pad = x_pad.view(*x_pad.size()[:-2], x_pad.size(-1), x_pad.size(-2))\n",
    "    return x_pad\n",
    "\n",
    "def _skewv(x, pad):\n",
    "    B, C, M, L = x.size()\n",
    "    x = F.pad(x, (0, M+1), value=pad)\n",
    "    x = x.view(B, C, -1)\n",
    "    x = x[:, :, :-M]\n",
    "    x = x.view(B, C, M, M+L)\n",
    "    x= x[:,:,:,:-1]\n",
    "    return x\n",
    "\n",
    "\n",
    "def sliding_chunk_matmul(q, k, w, pad):\n",
    "    B,T,C = q.shape\n",
    "\n",
    "    assert T % (w * 2) == 0\n",
    "    assert q.shape == k.shape\n",
    "\n",
    "    chunk_count = T // w- 1\n",
    "\n",
    "    #q = q.transpose(1,2)\n",
    "    #k = k.transpose(1,2)\n",
    "\n",
    "    qchunk = _chunk(q, w)\n",
    "    kchunk = _chunk(k, w)\n",
    "\n",
    "    chunked_attn = torch.einsum('bcxd, bcyd->bcxy', (qchunk, kchunk))\n",
    "\n",
    "    diag_chunk_attn = _skew(chunked_attn, dir=(0,0,0,1), pad=pad)\n",
    "    diag_attn = torch.zeros((B,chunk_count + 1, w, w * 2 + 1), device=q.device)\n",
    "    #diag_attn = diag_chunk_attn.new_empty((B,chunk_count + 1, w, w * 2 + 1))\n",
    "    \n",
    "    diag_attn[:, :-1, :, w:] = diag_chunk_attn[:, :, :w, :w+1]\n",
    "    diag_attn[:, -1, :, w:] = diag_chunk_attn[:, -1, w:, :w+1]\n",
    "\n",
    "    diag_attn[:, 1:, :, :w] = diag_chunk_attn[:, :, -(w+1):-1, w+1:]\n",
    "    diag_attn[:, 0, 1:w, 1:w] = diag_chunk_attn[:, 0, :w-1, 1-w:]\n",
    "\n",
    "    diag_attn = diag_attn.view(B, T, 2 * w +1)#.transpose(2, 1)\n",
    "\n",
    "    return diag_attn\n",
    "\n",
    "def sliding_chunk_matmul_v(attn, v, w):\n",
    "    B,T,C = v.shape\n",
    "\n",
    "    assert T % (w * 2) == 0\n",
    "    assert attn.size()[:2] == v.size()[:2]\n",
    "    assert attn.size(2) == 2 * w + 1\n",
    "\n",
    "    chunk_count = T // w- 1\n",
    "\n",
    "    #q = q.transpose(1,2)\n",
    "    #k = k.transpose(1,2)\n",
    "\n",
    "    chunk_prob = attn.reshape(B, T//w, w, 2*w+1)\n",
    "\n",
    "    pad_v = F.pad(v, (0,0,w,w), value=-1)\n",
    "\n",
    "    chunk_v_size = (B, chunk_count+1, 3*w, C)\n",
    "    chunk_v_stride = pad_v.stride()\n",
    "    chunk_v_stride = chunk_v_stride[0], w * chunk_v_stride[1], chunk_v_stride[1], chunk_v_stride[2]\n",
    "    chunk_v = pad_v.as_strided(size=chunk_v_size, stride=chunk_v_stride)\n",
    "\n",
    "    skew_prob = _skewv(chunk_prob, pad=0)\n",
    "    context = torch.einsum('bcwd,bcdh->bcwh', (skew_prob, chunk_v))\n",
    "\n",
    "    return context.view(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.0000,  ..., 1.6600, 1.8496, 1.8551],\n",
      "         [0.0000, 2.2924, 1.3036,  ..., 2.7999, 2.0772, 2.4124],\n",
      "         [0.0000, 1.2098, 2.4661,  ..., 2.8661, 2.9248, 2.9016],\n",
      "         ...,\n",
      "         [2.2204, 2.3682, 2.8499,  ..., 1.5737, 3.2960, 1.9400],\n",
      "         [2.4293, 2.8384, 2.3147,  ..., 3.3519, 2.2143, 2.5128],\n",
      "         [3.3137, 2.3607, 2.9765,  ..., 0.0000, 0.0000, 0.0000]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 17.6377,  20.0018,  18.1015,  25.6739,  18.4093,  20.7069,  22.0540,\n",
       "           19.2566,  20.7492,  23.0306],\n",
       "         [-16.5276, -14.6500, -16.9186,  -6.3146, -14.4112, -12.2407,  -9.9455,\n",
       "          -12.7977, -11.7430,  -9.5899],\n",
       "         [-21.1664, -20.7011, -22.0192,  -9.5137, -18.2107, -15.8548, -13.7926,\n",
       "          -17.1841, -14.9103, -12.9988],\n",
       "         [-37.9667, -36.4896, -39.0957, -29.2821, -35.3898, -35.1796, -33.2945,\n",
       "          -35.4808, -33.7225, -32.2936],\n",
       "         [-10.8993,  -8.9786, -12.6159,   1.5115,  -9.0796,  -6.0268,  -5.3072,\n",
       "           -7.8472,  -3.8100,  -2.9759],\n",
       "         [-15.6266, -15.9906, -17.2088,  -3.8856, -14.3705, -10.5381,  -8.7546,\n",
       "          -11.7928,  -8.9814,  -7.2145],\n",
       "         [-26.3584, -26.0478, -28.4386, -16.8168, -23.6308, -21.9483, -20.9290,\n",
       "          -21.5302, -20.1243, -20.6471],\n",
       "         [  3.0895,   4.5688,   2.5393,  18.2942,   7.9842,  10.7343,  12.5366,\n",
       "           11.6372,  12.9217,  13.3890],\n",
       "         [-15.1271, -15.3044, -17.2348,  -4.8900, -11.8522, -10.7421,  -8.4926,\n",
       "           -9.6485,  -8.3624,  -7.0490],\n",
       "         [  0.9088,  -1.0577,  -3.1139,  10.5170,   4.1415,   4.6528,   7.3958,\n",
       "            6.7561,   7.9657,   8.5364],\n",
       "         [ -9.4571,  -9.0279, -12.6909,  -1.4513,  -6.0129,  -6.1941,  -4.2119,\n",
       "           -5.1954,  -4.1125,  -3.7822],\n",
       "         [ 11.0162,  10.3666,   5.7626,  20.9807,  14.4369,  14.4888,  16.8959,\n",
       "           17.0368,  16.9064,  19.2008],\n",
       "         [ 11.4124,  11.8411,   6.6671,  22.2204,  15.6327,  13.3961,  16.3985,\n",
       "           17.4539,  16.5973,  20.1439],\n",
       "         [ 16.6328,  15.7641,  11.8781,  27.4828,  22.7998,  19.5041,  22.9794,\n",
       "           22.6829,  22.5425,  25.3697],\n",
       "         [ 19.0794,  19.0532,  15.8827,  30.2097,  26.1256,  21.3389,  26.6324,\n",
       "           26.1120,  25.1023,  28.5616],\n",
       "         [ 13.7718,  12.6006,  10.5920,  21.9082,  19.2032,  15.9962,  18.7000,\n",
       "           18.0156,  18.5492,  20.9657],\n",
       "         [ 23.1244,  23.2494,  20.8956,  30.7653,  28.5413,  24.1723,  27.8285,\n",
       "           26.2840,  26.2792,  30.0754],\n",
       "         [ 30.1527,  29.3554,  26.7610,  38.8720,  35.9753,  31.6248,  35.5581,\n",
       "           33.3594,  33.4631,  37.5591],\n",
       "         [ 44.6818,  42.1900,  40.0345,  55.3772,  52.9039,  45.4835,  50.6876,\n",
       "           48.6971,  48.4692,  54.7715],\n",
       "         [ 24.3118,  21.8824,  20.7782,  30.8226,  28.7825,  23.8572,  26.5551,\n",
       "           26.4835,  25.1450,  29.2268],\n",
       "         [ 25.9261,  24.7225,  24.0540,  31.2849,  31.0448,  25.0724,  27.2934,\n",
       "           27.9425,  25.6320,  30.2222],\n",
       "         [ 55.9200,  54.2941,  52.4018,  64.1353,  64.0554,  54.8986,  58.2348,\n",
       "           58.1861,  55.0347,  61.9142],\n",
       "         [ 54.8294,  54.8537,  53.8948,  65.1316,  65.2939,  55.8846,  60.1328,\n",
       "           58.6887,  55.7022,  63.7602],\n",
       "         [ 65.7794,  65.5774,  61.9581,  75.6350,  75.2277,  64.4938,  67.2968,\n",
       "           69.1798,  64.5308,  74.0402],\n",
       "         [ 55.2137,  55.3013,  52.0667,  63.4399,  63.8702,  54.9375,  57.2659,\n",
       "           57.1682,  54.3084,  61.6234],\n",
       "         [ 45.2490,  44.9873,  42.6655,  52.7524,  52.4100,  45.0347,  45.8232,\n",
       "           46.5733,  44.0656,  51.1619],\n",
       "         [ 72.4920,  71.0476,  67.4923,  83.5737,  84.8752,  72.3324,  72.4852,\n",
       "           78.6903,  69.2649,  80.2181],\n",
       "         [ 59.2102,  54.5564,  54.8493,  65.7494,  68.1596,  57.8389,  57.3852,\n",
       "           62.8290,  55.4593,  63.5300],\n",
       "         [ 55.1222,  50.9954,  49.4176,  61.0188,  62.9665,  52.7330,  53.5484,\n",
       "           58.0211,  49.6316,  58.5354],\n",
       "         [ 50.5595,  45.2046,  43.4744,  54.8850,  58.6710,  49.1118,  48.4822,\n",
       "           52.3064,  44.0708,  51.5015],\n",
       "         [ 46.7441,  40.6667,  39.5126,  51.1162,  53.3772,  44.5024,  42.9965,\n",
       "           48.8344,  41.4825,  47.8399],\n",
       "         [ 65.7239,  57.0803,  57.0781,  68.9747,  73.3736,  62.4935,  60.3182,\n",
       "           64.7675,  58.7681,  64.9947],\n",
       "         [ 33.0227,  28.6080,  28.7677,  35.6553,  38.8726,  32.5005,  31.7590,\n",
       "           34.5411,  29.7799,  32.6117],\n",
       "         [ 20.5899,  17.3736,  15.0353,  23.4597,  26.1139,  20.3339,  17.8120,\n",
       "           20.7328,  18.3613,  19.7050],\n",
       "         [ 37.8699,  33.9652,  31.4933,  40.4292,  44.7762,  36.1441,  33.8272,\n",
       "           37.9488,  36.0466,  37.7973],\n",
       "         [ 14.5524,  11.5302,  11.2272,  16.4162,  19.4601,  13.8443,  11.7050,\n",
       "           14.1210,  13.2866,  15.0683],\n",
       "         [ 21.5410,  17.4048,  17.6704,  24.3539,  28.3173,  22.2235,  19.7210,\n",
       "           20.0578,  18.7507,  23.2268],\n",
       "         [  8.8425,   3.1057,   3.7899,   9.3366,  15.5497,   8.4494,   5.2920,\n",
       "            8.0366,   7.0781,   8.4087],\n",
       "         [ 45.4010,  37.1779,  36.3641,  48.1269,  53.1129,  43.3849,  39.2384,\n",
       "           41.9776,  44.3552,  47.5223],\n",
       "         [  6.7156,   1.8025,   1.0529,   5.9392,  10.2232,   3.9636,   2.1757,\n",
       "            5.2847,   4.8571,   5.4057],\n",
       "         [-12.2531, -16.0983, -18.2830, -12.4393,  -7.3402, -14.1646, -16.0126,\n",
       "          -13.0115, -12.7313, -12.6295],\n",
       "         [ 23.5666,  16.2871,  12.4298,  22.2970,  31.2123,  20.1180,  18.0829,\n",
       "           20.2699,  21.8496,  22.0543],\n",
       "         [-12.9033, -17.2063, -20.2419, -12.3319,  -6.9877, -13.1223, -15.1350,\n",
       "          -13.2917, -12.6325, -13.5508],\n",
       "         [ 20.6088,  12.4032,  10.8413,  19.4189,  27.2780,  18.3658,  16.4067,\n",
       "           19.4019,  19.8514,  18.5647],\n",
       "         [-21.7334, -25.9940, -25.3754, -23.2668, -18.6120, -23.9108, -24.1841,\n",
       "          -22.3893, -23.7908, -22.5664],\n",
       "         [ -1.7999,  -8.0457,  -9.7348,  -4.6065,   3.6206,  -4.1029,  -6.1245,\n",
       "           -3.0727,  -5.4443,  -4.4447],\n",
       "         [-15.7311, -21.6258, -21.9685, -17.5168, -11.3076, -18.6334, -18.9261,\n",
       "          -17.1594, -19.3799, -16.3389],\n",
       "         [-11.8997, -16.6988, -17.1158, -14.7637,  -6.6684, -15.6176, -14.0802,\n",
       "          -14.0455, -15.9604, -13.1107],\n",
       "         [-21.4612, -26.2831, -26.8326, -22.3177, -14.4315, -24.5219, -23.3103,\n",
       "          -21.3851, -24.2098, -21.3880],\n",
       "         [ 38.8913,  33.4252,  33.0858,  35.7832,  46.1721,  33.6752,  36.2030,\n",
       "           38.7918,  35.0383,  37.1869]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.rand(1,8,1,384)\n",
    "k = torch.rand(1,8,1,384)\n",
    "q = torch.rand(1,8,1,384)\n",
    "\n",
    "v2 = torch.rand(1,50,10)\n",
    "k2 = torch.rand(1,50,10)\n",
    "q2 = torch.rand(1,50,10)\n",
    "\n",
    "#sliding_chunks_matmul_qk(q,v, 2, 0)\n",
    "attn = sliding_chunk_matmul(q2, k2, win_size, 0)\n",
    "print(attn)\n",
    "sliding_chunk_matmul_v(attn, v2,win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attnSlidingWindow(nn.Module):\n",
    "    '''\n",
    "        Attention(Q, K, V ) = softmax( QK^T/âˆšd_k)V \n",
    "    \n",
    "    '''\n",
    "    #Takes number of embedded, head_size, context length\n",
    "    def __init__(self, embn, hdim, con_l, drop=0.0):\n",
    "\n",
    "        super(attnSlidingWindow, self).__init__()\n",
    "        #dim is (d_k) when sqrt'd it is meant to counter small gradients in large sets of queries and keys\n",
    "        self.k = nn.Linear(embn, hdim, bias=False)\n",
    "        self.q = nn.Linear(embn, hdim, bias=False)\n",
    "        self.v = nn.Linear(embn, hdim, bias=False)\n",
    "        self.d_k = np.sqrt(hdim)\n",
    "\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(con_l,con_l)))\n",
    "        #Simple drop out \n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x, ret_att=False):\n",
    "        #batch X length X dim\n",
    "        B,T,C = x.shape\n",
    "        k = self.k(x)\n",
    "        q = self.q(x)\n",
    "\n",
    "        n = sliding_chunk_matmul(q, k, win_size, 0) * k.shape[-1]**-0.5\n",
    "        n = n.masked_fill(self.mask[:T,:n.shape[-1]]==0, float('-inf'))\n",
    "        #Drop out referenced later in paper but not in original diagram\n",
    "        att = self.drop(F.softmax(n, dim=-1))\n",
    "\n",
    "        v = self.v(x)\n",
    "\n",
    "        out = sliding_chunk_matmul_v(att, v, win_size)\n",
    "        if ret_att:\n",
    "            return out, att \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.2500e+00, -1.2500e+00, -1.2500e+00, -1.2500e+00, -1.2500e+00,\n",
      "          -1.2500e+00, -1.2500e+00, -1.2500e+00, -1.2500e+00, -1.2500e+00],\n",
      "         [-1.2500e+00, -1.2500e+00, -1.2500e+00, -1.2500e+00, -1.2500e+00,\n",
      "          -1.2500e+00, -1.2500e+00, -1.2500e+00, -1.2500e+00, -1.2500e+00],\n",
      "         [-1.2500e+00, -1.2500e+00, -1.2500e+00, -1.2500e+00, -1.2500e+00,\n",
      "          -1.2500e+00, -1.2500e+00, -1.2500e+00, -1.2500e+00, -1.2500e+00],\n",
      "         [-9.9393e-01, -9.9393e-01, -9.9393e-01, -9.9393e-01, -9.9393e-01,\n",
      "          -9.9393e-01, -9.9393e-01, -9.9393e-01, -9.9393e-01, -9.9393e-01],\n",
      "         [-8.3276e-01, -8.3276e-01, -8.3276e-01, -8.3276e-01, -8.3276e-01,\n",
      "          -8.3276e-01, -8.3276e-01, -8.3276e-01, -8.3276e-01, -8.3276e-01],\n",
      "         [-7.2674e-01, -7.2674e-01, -7.2674e-01, -7.2674e-01, -7.2674e-01,\n",
      "          -7.2674e-01, -7.2674e-01, -7.2674e-01, -7.2674e-01, -7.2674e-01],\n",
      "         [-1.0979e+00, -1.0979e+00, -1.0979e+00, -1.0979e+00, -1.0979e+00,\n",
      "          -1.0979e+00, -1.0979e+00, -1.0979e+00, -1.0979e+00, -1.0979e+00],\n",
      "         [-1.1133e+00, -1.1133e+00, -1.1133e+00, -1.1133e+00, -1.1133e+00,\n",
      "          -1.1133e+00, -1.1133e+00, -1.1133e+00, -1.1133e+00, -1.1133e+00],\n",
      "         [-1.1316e+00, -1.1316e+00, -1.1316e+00, -1.1316e+00, -1.1316e+00,\n",
      "          -1.1316e+00, -1.1316e+00, -1.1316e+00, -1.1316e+00, -1.1316e+00],\n",
      "         [-7.9940e-01, -7.9940e-01, -7.9940e-01, -7.9940e-01, -7.9940e-01,\n",
      "          -7.9940e-01, -7.9940e-01, -7.9940e-01, -7.9940e-01, -7.9940e-01],\n",
      "         [-9.4085e-01, -9.4085e-01, -9.4085e-01, -9.4085e-01, -9.4085e-01,\n",
      "          -9.4085e-01, -9.4085e-01, -9.4085e-01, -9.4085e-01, -9.4085e-01],\n",
      "         [-5.7911e-01, -5.7911e-01, -5.7911e-01, -5.7911e-01, -5.7911e-01,\n",
      "          -5.7911e-01, -5.7911e-01, -5.7911e-01, -5.7911e-01, -5.7911e-01],\n",
      "         [-1.0329e+00, -9.7936e-01, -1.1341e+00, -9.3281e-01, -1.1380e+00,\n",
      "          -9.2370e-01, -9.6053e-01, -1.0284e+00, -8.3885e-01, -1.0012e+00],\n",
      "         [-8.8717e-01, -8.6029e-01, -1.0443e+00, -8.0039e-01, -1.0375e+00,\n",
      "          -7.7815e-01, -8.4445e-01, -8.8006e-01, -6.2638e-01, -8.5534e-01],\n",
      "         [-7.9818e-01, -8.0620e-01, -1.0120e+00, -6.9257e-01, -1.0246e+00,\n",
      "          -7.0120e-01, -7.4186e-01, -7.7827e-01, -4.9271e-01, -7.7242e-01],\n",
      "         [-6.3549e-01, -5.8441e-01, -9.5039e-01, -4.9698e-01, -9.3566e-01,\n",
      "          -5.0824e-01, -5.1052e-01, -5.9593e-01, -2.5241e-01, -6.1431e-01],\n",
      "         [-5.6032e-01, -5.1932e-01, -9.1269e-01, -3.8732e-01, -9.1097e-01,\n",
      "          -3.9889e-01, -4.3502e-01, -5.3941e-01, -7.7871e-02, -4.9032e-01],\n",
      "         [-4.6086e-01, -4.3665e-01, -9.1469e-01, -3.5519e-01, -9.0294e-01,\n",
      "          -3.2091e-01, -3.5499e-01, -4.9958e-01,  8.4937e-02, -4.4486e-01],\n",
      "         [-4.8167e-01, -4.4294e-01, -7.7405e-01, -3.3832e-01, -8.2070e-01,\n",
      "          -3.2501e-01, -3.7972e-01, -4.6039e-01, -1.7509e-02, -4.0683e-01],\n",
      "         [-4.1651e-01, -4.0279e-01, -8.8262e-01, -2.7465e-01, -1.0046e+00,\n",
      "          -2.6959e-01, -2.9783e-01, -4.2945e-01,  2.1722e-01, -3.6382e-01],\n",
      "         [-2.8274e-01, -2.6699e-01, -7.4888e-01, -1.4578e-01, -8.5473e-01,\n",
      "          -1.3992e-01, -1.4132e-01, -2.7480e-01,  3.4390e-01, -2.5299e-01],\n",
      "         [-2.9915e-01, -3.2063e-01, -6.6913e-01, -2.0391e-01, -7.8233e-01,\n",
      "          -2.1233e-01, -2.1903e-01, -2.6531e-01,  1.9696e-01, -2.8642e-01],\n",
      "         [-2.7125e-01, -2.1958e-01, -7.0326e-01, -1.7259e-01, -7.5780e-01,\n",
      "          -1.2731e-01, -1.5737e-01, -2.6381e-01,  3.2755e-01, -2.3335e-01],\n",
      "         [-2.6585e-01, -2.4510e-01, -8.3844e-01, -1.3696e-01, -9.4504e-01,\n",
      "          -8.0840e-02, -1.4361e-01, -2.0829e-01,  5.0283e-01, -2.3766e-01],\n",
      "         [-1.9384e-01, -1.7367e-01, -6.8190e-01, -9.6012e-02, -7.9867e-01,\n",
      "          -1.8965e-02, -7.0458e-02, -1.1830e-01,  4.5552e-01, -1.7828e-01],\n",
      "         [-1.7571e-01, -1.6965e-01, -7.3749e-01, -7.5767e-02, -8.3992e-01,\n",
      "          -1.4315e-02, -6.8736e-02, -1.3928e-01,  5.2624e-01, -1.4092e-01],\n",
      "         [-2.0299e-01, -1.6322e-01, -7.5984e-01, -9.7518e-02, -8.9920e-01,\n",
      "           2.6562e-04, -1.0052e-01, -1.3217e-01,  5.3152e-01, -1.5104e-01],\n",
      "         [-1.6845e-01, -1.8814e-01, -6.8677e-01, -7.6356e-02, -8.1851e-01,\n",
      "          -3.1506e-02, -8.1970e-02, -1.2331e-01,  4.7589e-01, -1.7947e-01],\n",
      "         [-1.6897e-01, -1.5543e-01, -5.6086e-01, -4.5313e-02, -7.0193e-01,\n",
      "          -1.7550e-02, -7.1567e-02, -9.7136e-02,  4.0299e-01, -1.4292e-01],\n",
      "         [-1.4864e-01, -1.5997e-01, -6.8848e-01, -1.1246e-01, -7.5686e-01,\n",
      "           1.5607e-02, -4.4125e-02, -1.3506e-01,  4.8013e-01, -1.7256e-01],\n",
      "         [-1.5046e-01, -1.5584e-01, -6.0080e-01, -1.0730e-01, -7.9323e-01,\n",
      "          -1.6315e-02, -9.8490e-02, -1.3713e-01,  4.3065e-01, -1.5544e-01],\n",
      "         [-1.5665e-01, -1.4024e-01, -6.5215e-01, -1.3068e-01, -8.1075e-01,\n",
      "          -8.3630e-03, -4.6598e-02, -1.3750e-01,  5.0561e-01, -1.5806e-01],\n",
      "         [-1.7956e-01, -1.7439e-01, -6.7333e-01, -1.0770e-01, -8.4705e-01,\n",
      "           2.1850e-02, -7.1787e-02, -1.6981e-01,  4.7859e-01, -1.7462e-01],\n",
      "         [-1.4113e-01, -1.3736e-01, -6.0533e-01, -8.6624e-02, -7.4129e-01,\n",
      "           5.5076e-02, -8.8002e-02, -1.1450e-01,  4.7066e-01, -1.5100e-01],\n",
      "         [-1.9564e-01, -1.7672e-01, -7.1736e-01, -1.0304e-01, -8.8984e-01,\n",
      "           2.1747e-02, -1.1948e-01, -1.5739e-01,  5.3151e-01, -1.9927e-01],\n",
      "         [-1.5054e-01, -1.1488e-01, -5.3963e-01, -8.8678e-02, -6.9330e-01,\n",
      "           1.4843e-02, -8.5703e-02, -1.2571e-01,  4.3606e-01, -1.4788e-01],\n",
      "         [-1.7344e-01, -1.5826e-01, -7.0359e-01, -9.0190e-02, -8.5030e-01,\n",
      "           4.9770e-02, -1.0079e-01, -1.6675e-01,  5.2079e-01, -1.8683e-01],\n",
      "         [-2.1091e-01, -1.9489e-01, -5.9984e-01, -1.1093e-01, -6.9703e-01,\n",
      "          -5.2117e-02, -1.4988e-01, -1.9157e-01,  2.9790e-01, -2.0986e-01],\n",
      "         [-2.9117e-01, -2.7151e-01, -7.4228e-01, -1.9333e-01, -8.9833e-01,\n",
      "          -9.1624e-02, -2.3062e-01, -2.6780e-01,  3.3374e-01, -2.9639e-01],\n",
      "         [-2.8508e-01, -2.5078e-01, -7.1520e-01, -1.8808e-01, -8.5106e-01,\n",
      "          -9.9890e-02, -2.1584e-01, -2.6994e-01,  2.9773e-01, -2.8589e-01],\n",
      "         [-3.0572e-01, -2.9168e-01, -7.9451e-01, -2.1400e-01, -9.0285e-01,\n",
      "          -1.0918e-01, -2.5006e-01, -2.8927e-01,  3.3571e-01, -3.0884e-01],\n",
      "         [-3.3537e-01, -3.4998e-01, -6.9021e-01, -2.9454e-01, -7.4796e-01,\n",
      "          -1.9594e-01, -3.1535e-01, -3.3660e-01,  1.1362e-01, -3.6127e-01],\n",
      "         [-2.6646e-01, -2.8216e-01, -6.8265e-01, -2.5136e-01, -7.0593e-01,\n",
      "          -1.6026e-01, -2.4840e-01, -2.8951e-01,  1.4588e-01, -2.9580e-01],\n",
      "         [-3.7652e-01, -3.5099e-01, -6.9967e-01, -3.1588e-01, -7.7274e-01,\n",
      "          -2.1971e-01, -3.3448e-01, -3.7100e-01,  2.4669e-02, -3.8963e-01],\n",
      "         [-4.5536e-01, -4.4309e-01, -8.1478e-01, -4.0866e-01, -8.9230e-01,\n",
      "          -3.2105e-01, -4.1714e-01, -4.6092e-01, -3.7298e-02, -4.6800e-01],\n",
      "         [-5.3350e-01, -5.4663e-01, -8.3568e-01, -4.9039e-01, -8.7962e-01,\n",
      "          -4.2516e-01, -5.2982e-01, -5.4109e-01, -1.6070e-01, -5.5446e-01],\n",
      "         [-5.1075e-01, -5.0879e-01, -7.9151e-01, -4.4692e-01, -8.5556e-01,\n",
      "          -3.9544e-01, -4.8486e-01, -5.1770e-01, -1.8771e-01, -5.1723e-01],\n",
      "         [-5.1632e-01, -5.0652e-01, -7.9418e-01, -4.5651e-01, -8.4325e-01,\n",
      "          -4.0006e-01, -5.0981e-01, -5.2956e-01, -1.5752e-01, -5.2254e-01],\n",
      "         [-6.2750e-01, -6.2550e-01, -9.2221e-01, -5.7019e-01, -9.8972e-01,\n",
      "          -4.9385e-01, -5.8195e-01, -6.4258e-01, -2.5251e-01, -6.3837e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Scaled dot product attention testing\n",
    "#dim should be size of q and k\n",
    "\n",
    "\n",
    "windowattn = attnSlidingWindow(384, 10, 100, drop=0.2)\n",
    "\n",
    "v = torch.rand(1,50,384)\n",
    "\n",
    "\n",
    "\n",
    "print(windowattn(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, dims, embn, con_l, dropout=0.0):\n",
    "        super(multiHeadedAttention, self).__init__()\n",
    "        #d_k=d_v = dims/h\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.attn = nn.ModuleList([attnSlidingWindow(embn, dims, con_l) for _ in range(n_heads)])\n",
    "        #Final linear layer after concat and attention\n",
    "        self.fc = nn.Linear(n_heads*dims, embn)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.attn], dim=-1)\n",
    "        out = self.drop(self.fc(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.1719, -0.8455, -0.2509,  ...,  0.0000, -0.0000,  0.4248],\n",
      "         [-2.1719, -0.8455, -0.2509,  ...,  1.2850, -0.4226,  0.4248],\n",
      "         [-2.1719, -0.0000, -0.2509,  ...,  1.2850, -0.4226,  0.0000],\n",
      "         ...,\n",
      "         [-0.0000, -0.2245, -0.0643,  ...,  0.4177, -0.2174,  0.2085],\n",
      "         [-0.0000, -0.2608, -0.0000,  ...,  0.4212, -0.2153,  0.2435],\n",
      "         [-0.0000, -0.0000, -0.0838,  ...,  0.5351, -0.2666,  0.2365]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#heads, d_model, d_km d_v as per the paper\n",
    "torch.manual_seed(1337)\n",
    "multiHead = multiHeadedAttention(6, 50, 384, 512, dropout=0.2)\n",
    "\n",
    "#batches, dims, dimensionalityxn_heads\n",
    "\n",
    "v = torch.rand(1,50,384)\n",
    "\n",
    "\n",
    "print(multiHead(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionFeedFoward(nn.Module):\n",
    "    def __init__(self, inp, hid, drop=0.0):\n",
    "        super(positionFeedFoward, self).__init__()\n",
    "        self.w1 = nn.Linear(inp,4*hid)\n",
    "        self.w2 = nn.Linear(4*hid,inp)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.w2(F.relu(self.w1(x)))\n",
    "        x = self.drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''Combinds MultiHeadedAttention and FeeForward, three layers'''\n",
    "    def __init__(self, nheads, embn, con_l, drop=0.0):\n",
    "        super(Decoder, self).__init__()\n",
    "        head_size = embn // nheads\n",
    "        self.slf_attn = multiHeadedAttention(nheads, head_size,embn, con_l, dropout=drop)\n",
    "        \n",
    "        self.ffn = positionFeedFoward(embn, embn, drop=drop)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embn)\n",
    "        self.norm2 = nn.LayerNorm(embn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.slf_attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1804, -0.3295, -0.4308,  ...,  2.0671,  0.9802,  0.9299],\n",
       "         [ 0.9376, -0.4714, -0.2999,  ...,  1.7397,  1.5895,  0.6495],\n",
       "         [ 1.4974,  0.0910, -0.2238,  ...,  1.9109,  1.7711,  1.0007],\n",
       "         ...,\n",
       "         [ 1.0018, -0.1989, -0.3057,  ...,  1.8529,  1.4204,  0.9704],\n",
       "         [ 1.0441,  0.1497,  0.5606,  ...,  0.6702,  1.6538,  0.8308],\n",
       "         [ 0.9663, -0.2711, -0.1064,  ...,  1.7594,  1.4071,  0.4604]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#heads, d_model, d_km d_v as per the paper\n",
    "enc = Decoder(8, 64, 512)\n",
    "#batches, dims, dimensionalityxn_heads\n",
    "\n",
    "v = torch.rand(1,50,64)\n",
    "\n",
    "\n",
    "enc(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class languageModel(nn.Module):\n",
    "    '''Decoder model'''\n",
    "    def __init__(\n",
    "            self, n_vocab, embn, n_layers, n_head, dropout=0.2 , con_l=200\n",
    "    ):\n",
    "        super(languageModel, self).__init__()\n",
    "        self.con_l = con_l\n",
    "        self.word_emb = nn.Embedding(n_vocab, embn)\n",
    "        self.pos_enc = nn.Embedding(con_l, embn)\n",
    "        self.stack = nn.Sequential(\n",
    "            *[Decoder( n_head, embn, con_l, drop=dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "       \n",
    "        self.layer_norm = nn.LayerNorm(embn)\n",
    "        self.fc = nn.Linear(embn, n_vocab)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, tar=None):\n",
    "        #batch, time\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok = self.word_emb(x)\n",
    "        pos = self.pos_enc(torch.arange(T, device=device))\n",
    "        x = tok + pos\n",
    "        x = self.stack(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        if tar is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            tar = tar.view(B*T)\n",
    "            loss = F.cross_entropy(logits, tar)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x, max_length):\n",
    "        #x is a BxT array of in current context\n",
    "        fullout=x\n",
    "        for _ in range(max_length):\n",
    "            x_cond = x[:, -win_size*2:]\n",
    "            logits, loss = self(x_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            x_next = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat((x, x_next), dim=1)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits = model(X)[0]\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = Y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 14\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 41\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[1;32m     40\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[0;32m---> 41\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     42\u001b[0m     B, T, C \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     43\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mT, C)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 34\u001b[0m, in \u001b[0;36mlanguageModel.forward\u001b[0;34m(self, x, tar)\u001b[0m\n\u001b[1;32m     32\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_enc(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m tok \u001b[38;5;241m+\u001b[39m pos\n\u001b[0;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n\u001b[1;32m     36\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[42], line 14\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslf_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 16\u001b[0m, in \u001b[0;36mmultiHeadedAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h(x) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out))\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[39], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out))\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 27\u001b[0m, in \u001b[0;36mattnSlidingWindow.forward\u001b[0;34m(self, x, ret_att)\u001b[0m\n\u001b[1;32m     24\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq(x)\n\u001b[1;32m     26\u001b[0m n \u001b[38;5;241m=\u001b[39m sliding_chunk_matmul(q, k, win_size, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m---> 27\u001b[0m n \u001b[38;5;241m=\u001b[39m n\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask[:T,:n\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-inf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#Drop out referenced later in paper but not in original diagram\u001b[39;00m\n\u001b[1;32m     29\u001b[0m att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(F\u001b[38;5;241m.\u001b[39msoftmax(n, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = languageModel(vocab_size,  384,6, 6, con_l=100\n",
    "    )\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "#print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(next(m.parameters()).is_cuda)\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    #B, T, C = logits.shape\n",
    "    #logits = logits.view(B*T, C)\n",
    "    #targets = yb.view(B*T)\n",
    "    #loss = F.cross_entropy(logits, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 50), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_length=500)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cont \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "Cell \u001b[0;32mIn[29], line 53\u001b[0m, in \u001b[0;36mlanguageModel.generate\u001b[0;34m(self, x, max_length)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length):\n\u001b[1;32m     52\u001b[0m     x_cond \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m-\u001b[39mwin_size:]\n\u001b[0;32m---> 53\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[1;32m     55\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 34\u001b[0m, in \u001b[0;36mlanguageModel.forward\u001b[0;34m(self, x, tar)\u001b[0m\n\u001b[1;32m     32\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_enc(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m tok \u001b[38;5;241m+\u001b[39m pos\n\u001b[0;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n\u001b[1;32m     36\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslf_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 16\u001b[0m, in \u001b[0;36mmultiHeadedAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h(x) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out))\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[24], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out))\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 26\u001b[0m, in \u001b[0;36mattnSlidingWindow.forward\u001b[0;34m(self, x, ret_att)\u001b[0m\n\u001b[1;32m     23\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk(x)\n\u001b[1;32m     24\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq(x)\n\u001b[0;32m---> 26\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[43msliding_chunk_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     27\u001b[0m n \u001b[38;5;241m=\u001b[39m n\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask[:T,:n\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#Drop out referenced later in paper but not in original diagram\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 30\u001b[0m, in \u001b[0;36msliding_chunk_matmul\u001b[0;34m(q, k, w, pad)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msliding_chunk_matmul\u001b[39m(q, k, w, pad):\n\u001b[1;32m     28\u001b[0m     B,T,C \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m T \u001b[38;5;241m%\u001b[39m (w \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m k\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     33\u001b[0m     chunk_count \u001b[38;5;241m=\u001b[39m T \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m w\u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cont = torch.zeros((1, 50), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(cont, max_length=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
